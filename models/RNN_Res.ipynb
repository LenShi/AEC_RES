{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAPSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audio_dir):\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        return signal, label\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a class for linear layers \n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,dropOut_p):\n",
    "        super.__init_(self)\n",
    "        self.dense=nn.Linear(input_size,output_size,bias=True)\n",
    "        self.activation=nn.Tanh()\n",
    "        self.dropOut=nn.Dropout(p=dropOut_p,inplace=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=self.dense(x)\n",
    "        y=self.activation(y)\n",
    "        y=self.dropOut()\n",
    "\n",
    "        return y\n",
    "\n",
    "#class for convolutional layers\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel_size,stride,padding,dropOut_p):\n",
    "        super.__init_(self)\n",
    "        self.conv=nn.Conv2d(in_ch,out_ch,kernel_size, stride=stride,padding=padding)\n",
    "        self.activation=nn.ReLU()\n",
    "        self.dropOut=nn.Dropout(p=dropOut_p,inplace=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=self.conv(x)\n",
    "        y=self.activation(y)\n",
    "        y=self.dropOut()\n",
    "\n",
    "        return y\n",
    "\n",
    "def Loss_SDR():\n",
    "    pass\n",
    "\n",
    "\n",
    "class RES(nn.Model):\n",
    "    # K FFT bins, L samples in time, M LSTM size\n",
    "    def __init__(K,L,M):\n",
    "        super.__init__(RES,)\n",
    "        output_size=128\n",
    "        dense1=nn.DenseLayer()\n",
    "        lstm1=nn.LSTM()\n",
    "        dense1=nn.DenseLayer()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
