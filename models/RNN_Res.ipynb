{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAPSDatasetHelper():\n",
    "    #Get the dataset dictionary \n",
    "    def get_file_descriptors(self,dirpath):\n",
    "        directory={}\n",
    "        dataset_path=self.dir\n",
    "        cwd= os.getcwd()\n",
    "        for i , (dirpath, dirname, filename) in enumerate(os.walk(dataset_path)):\n",
    "            if(dirpath!=dataset_path):\n",
    "                dirname=dirpath.split(\"/\")[-1]\n",
    "                files={}\n",
    "                file_list=[]\n",
    "                index=0\n",
    "                for file in filename:\n",
    "                    filepath = os.path.join( dirpath, file)\n",
    "                    if ( (filepath.endswith('.wav'))):\n",
    "                        if(file.startswith('.')):\n",
    "                            pass\n",
    "                        else:\n",
    "                            file_list.append(filepath)\n",
    "                file_list.sort()\n",
    "                if(len(file_list)>0):\n",
    "                    for filepath in file_list:\n",
    "                        files[index]=filepath\n",
    "                        index+=1\n",
    "                    directory[dirname]=files\n",
    "        return directory\n",
    "\n",
    "    #initialization \n",
    "    def __init__(self):\n",
    "        self.sample_rate=8000\n",
    "        self.dir= \"../dataset_daps/daps\"\n",
    "        self.dataset_dict=self.get_file_descriptors(self.dir)\n",
    "\n",
    "        #stft config\n",
    "        #frame size in ms\n",
    "        self.framesize=25\n",
    "        self.fft_len=self.sample_rate*self.framesize//1000\n",
    "        self.window_size=self.fft_len\n",
    "        self.hop_len=self.fft_len//2\n",
    "\n",
    "        indx=2\n",
    "        self.keys={}\n",
    "        for key  in self.dataset_dict.keys():\n",
    "            if(key==\"produced\"):\n",
    "                self.keys[1]=key\n",
    "            else:\n",
    "                self.keys[indx]=key\n",
    "                indx+=1\n",
    "\n",
    "        self.num_files_per_category=len(self.dataset_dict[\"produced\"].keys())\n",
    "\n",
    "    #get the indexed file and sample rate\n",
    "    def get_indxd_file(self,indx,isLabel=False):\n",
    "        if(isLabel):\n",
    "            category=self.keys[1]\n",
    "        else:\n",
    "            category=self.keys[np.random.randint(2,len(self.keys))]\n",
    "        data,sr= librosa.load(self.dataset_dict[category][indx])\n",
    "        Id= self.dataset_dict[category][indx].split(\"/\")[-1].split('.')[0]\n",
    "        return (data,sr, Id)\n",
    "\n",
    "    def resample_audio(self,file,sr):\n",
    "        out = librosa.resample(file, orig_sr=sr, target_sr=self.sample_rate)\n",
    "        return out\n",
    "\n",
    "    #get the train data and label at given index \n",
    "    def get_data(self,indx):\n",
    "        data,sr,Id_data = self.get_indxd_file(indx)\n",
    "        label,sr,Id_label = self.get_indxd_file(indx,True)\n",
    "        if(sr == self.sample_rate ):\n",
    "            pass\n",
    "        else:\n",
    "            data= self.resample_audio(data,sr)\n",
    "            label= self.resample_audio(label,sr)\n",
    "\n",
    "        return (data,label,Id_data,Id_label)\n",
    "\n",
    "\n",
    "    #get stft frames with 50% overlap\n",
    "    def getFeatures(self,file):\n",
    "\n",
    "        n_fft = self.fft_len\n",
    "        win_length = self.window_size\n",
    "        hop_length = self.hop_len\n",
    "\n",
    "        # define transformation\n",
    "        spectrogram = T.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "        )\n",
    "        # Perform transformation\n",
    "        waveform=torch.from_numpy(file)\n",
    "        spec = spectrogram(waveform)\n",
    "\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAPS(Dataset):\n",
    "    def __init__(self):\n",
    "        #super().__init__(self)\n",
    "        self.daps= DAPSDatasetHelper()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data,label,_,_=self.daps.get_data(index)\n",
    "        data_spec=self.daps.getFeatures(data)\n",
    "        label_spec=self.daps.getFeatures(label)\n",
    "        return (data_spec,label_spec)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.daps.keys)-1)*self.daps.num_files_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model classes\n",
    "# create a class for linear layers \n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,dropOut_p):\n",
    "        super().__init_(self)\n",
    "        self.dense=nn.Linear(input_size,output_size,bias=True)\n",
    "        self.activation=nn.Tanh()\n",
    "        self.dropOut=nn.Dropout(p=dropOut_p,inplace=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=self.dense(x)\n",
    "        y=self.activation(y)\n",
    "        y=self.dropOut()\n",
    "\n",
    "        return y\n",
    "\n",
    "#class for convolutional layers\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel_size,stride,padding,dropOut_p):\n",
    "        super().__init_(self)\n",
    "        self.conv=nn.Conv2d(in_ch,out_ch,kernel_size, stride=stride,padding=padding)\n",
    "        self.activation=nn.ReLU()\n",
    "        self.dropOut=nn.Dropout(p=dropOut_p,inplace=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=self.conv(x)\n",
    "        y=self.activation(y)\n",
    "        y=self.dropOut()\n",
    "\n",
    "        return y\n",
    "\n",
    "def Loss_SDR(yhat,y):\n",
    "    loss= 1\n",
    "\n",
    "#RNN model for Residual echo suppression\n",
    "class RES(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super().__init__(self)\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        self.fc1= DenseLayer(hidden_dim,output_dim,dropOut_p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN=60\n",
    "NUM_VAL=30\n",
    "NUM_TEST=10\n",
    "print_every = 100\n",
    "dataset_train = DAPS()\n",
    "loader_train = DataLoader(dataset_train, batch_size=1, num_workers=0,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        print(x.shape)\n",
    "        print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_fn, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    \n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: compute scores and loss\n",
    "        yhat = model_fn(x, params)\n",
    "        loss = Loss_SDR(yhat, y)\n",
    "\n",
    "        # Backward pass: PyTorch figures out which Tensors in the computational\n",
    "        # graph has requires_grad=True and uses backpropagation to compute the\n",
    "        # gradient of the loss with respect to these Tensors, and stores the\n",
    "        # gradients in the .grad attribute of each Tensor.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters. We don't want to backpropagate through the\n",
    "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "        # context manager to prevent a computational graph from being built.\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # Manually zero the gradients after running the backward pass\n",
    "                w.grad.zero_()\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            #check_accuracy_part2(loader_val, model_fn, params)\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f5c7ac77e593d0994949fe61f06040dedc773eb228097f0a804bc0d2d8d2f83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aec': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
