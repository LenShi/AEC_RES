{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "from torch.utils.data import sampler\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAPSDatasetHelper():\n",
    "    #initialization \n",
    "    def __init__(self):\n",
    "        self.sample_rate=8000\n",
    "        self.dir= \"./daps_dict2.json\"\n",
    "        openfile=open(self.dir, \"r\")\n",
    "        self.dataset_dict= json.load(openfile)\n",
    "        indx=2\n",
    "        self.keys={}\n",
    "        for key  in self.dataset_dict.keys():\n",
    "            if(key==\"produced\"):\n",
    "                self.keys[1]=key\n",
    "            else:\n",
    "                self.keys[indx]=key\n",
    "                indx+=1\n",
    "\n",
    "        self.num_files_per_category=len(self.dataset_dict[\"produced\"].keys())\n",
    "\n",
    "    #get the indexed file and sample rate\n",
    "    def get_indxd_file(self,indx,isLabel=False):\n",
    "        if(isLabel):\n",
    "            category=self.keys[1]\n",
    "        else:\n",
    "            #category=self.keys[np.random.randint(2,len(self.keys))]\n",
    "            #consider one category for now\n",
    "            category=self.keys[4]\n",
    "        data=cv2.imread(self.dataset_dict[category][str(indx)],0)\n",
    "        Id= self.dataset_dict[category][str(indx)].split(\"/\")[-1].split('.')[0]\n",
    "        return (data,Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAPS(Dataset):\n",
    "    def __init__(self):\n",
    "        #super().__init__(self)\n",
    "        self.daps= DAPSDatasetHelper()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data,id=self.daps.get_indxd_file(index)\n",
    "        label,id=self.daps.get_indxd_file(index,True)\n",
    "        return (data,label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.daps.keys)-1)*self.daps.num_files_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN=3000\n",
    "NUM_VAL=1000\n",
    "NUM_TEST=1000\n",
    "print_every = 100\n",
    "dataset_train = DAPS()\n",
    "loader_train = DataLoader(dataset_train, batch_size=8, num_workers=0,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "loader_val= DataLoader(dataset_train, batch_size=16, num_workers=0,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN,NUM_VAL+NUM_TRAIN)))\n",
    "loader_test = DataLoader(dataset_train, batch_size=16, num_workers=0,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_VAL+NUM_TRAIN, NUM_VAL+NUM_TRAIN+NUM_TEST)))\n",
    "\n",
    "helper= DAPSDatasetHelper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                kernel_size=(3,3),stride=1,padding=1)\n",
    "        #self.bnorm1=nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,stride=1,padding=1)\n",
    "        #self.bnorm2=nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, c_list):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,1),stride=(2,1))\n",
    "        self.blocks = nn.ModuleList([block(c_list[i], c_list[i+1]) for i in range(len(c_list)-1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        filters = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            #print(\"conv output\",x.shape)\n",
    "            filters.append(x)\n",
    "            x = self.pool(x)\n",
    "            #print(\"pool output\",x.shape)\n",
    "        return filters\n",
    "\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, c_list):\n",
    "        super().__init__()\n",
    "        self.up_cov = nn.ModuleList([nn.ConvTranspose2d(c_list[i], c_list[i+1], kernel_size=(2,1), stride=(2,1))\n",
    "                                                                    for i in range(len(c_list)-1)])\n",
    "        self.conv_blocks = nn.ModuleList([block(c_list[i], c_list[i+1]) for i in range(len(c_list)-1)])\n",
    "        \n",
    "\n",
    "    def forward(self, x, features):    \n",
    "        for i in range(len(self.conv_blocks)):\n",
    "            x = self.up_cov[i](x)\n",
    "           # print(\"up conv output\",features[i].shape, x.shape)\n",
    "            x = torch.cat([x, features[i]], dim=1)\n",
    "            x = self.conv_blocks[i](x)\n",
    "            #print(\"concat output\",x.shape)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c_list=[1, 16, 32, 64, 128]\n",
    "        self.enc_blocks = encoder(c_list)\n",
    "        self.dec_blocks = decoder(c_list[::-1][:-1])\n",
    "        self.last_layer = nn.Conv2d(16, 1, kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        filters = self.enc_blocks(x)\n",
    "        x = self.dec_blocks(filters[-1], filters[::-1][1:])\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNR loss where yhat is average power spectrum of de-nonlinearised/denoised signal/ clean signal power \n",
    "def Loss_SNR(yhat,y):\n",
    "    yhat=yhat.cpu().detach().numpy()\n",
    "    y=y.cpu().detach().numpy()\n",
    "    loss= np.mean(10*np.log(np.abs(yhat-y)/y))\n",
    "    loss=torch.from_numpy(loss)\n",
    "    loss=loss.to(device=device,dtype=dtype)\n",
    "    return loss\n",
    "\n",
    "#SNR loss where yhat is average power spectrum of de-nonlinearised/denoised signal/ clean signal power \n",
    "def Loss_MSE(yhat,y,lossfn):\n",
    "    loss=lossfn(yhat,y)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every=1\n",
    "def train_model(model, optimizer ,epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on DAPS.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    \n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    lossfn=nn.L1Loss()\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            \n",
    "            x=x.unsqueeze(1)\n",
    "            y=y.unsqueeze(1)  \n",
    "\n",
    "            #print(y.shape)          \n",
    "            #iterate over each stft frame \n",
    "            x=x.to(device=device, dtype=dtype)\n",
    "            y=y.to(device=device, dtype=dtype)\n",
    "\n",
    "            # Forward pass: compute scores and loss\n",
    "            yhat = model(x)\n",
    "            \n",
    "            loss = Loss_MSE(yhat, y, lossfn)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            # Update parameters. We don't want to backpropagate through the\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()/len(loader_train)))\n",
    "            #check_accuracy_part2(loader_val, model_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-10\n",
    "\n",
    "model = UNet()\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "#                     momentum=0.9, nesterov=True)\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 374, loss = 0.0605\n",
      "Epoch 1, Iteration 374, loss = 0.0459\n",
      "Epoch 2, Iteration 374, loss = 0.0633\n",
      "Epoch 3, Iteration 374, loss = 0.0603\n",
      "Epoch 4, Iteration 374, loss = 0.0879\n",
      "Epoch 5, Iteration 374, loss = 0.0214\n",
      "Epoch 6, Iteration 374, loss = 0.0562\n",
      "Epoch 7, Iteration 374, loss = 0.0846\n",
      "Epoch 8, Iteration 374, loss = 0.0879\n",
      "Epoch 9, Iteration 374, loss = 0.0930\n",
      "Epoch 10, Iteration 374, loss = 0.0924\n",
      "Epoch 11, Iteration 374, loss = 0.0588\n",
      "Epoch 12, Iteration 374, loss = 0.0613\n",
      "Epoch 13, Iteration 374, loss = 0.0804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/prasad/AEC/model/CNN_Res.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m train_model(model, optimizer,\u001b[39m30\u001b[39;49m)\n",
      "\u001b[1;32m/home/prasad/AEC/model/CNN_Res.ipynb Cell 9'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000009vscode-remote?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)  \u001b[39m# move the model parameters to CPU/GPU\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000009vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000009vscode-remote?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader_train):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000009vscode-remote?line=21'>22</a>\u001b[0m         x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000009vscode-remote?line=22'>23</a>\u001b[0m         y\u001b[39m=\u001b[39my\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)  \n",
      "File \u001b[0;32m~/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/prasad/AEC/model/CNN_Res.ipynb Cell 3'\u001b[0m in \u001b[0;36mDAPS.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000002vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000002vscode-remote?line=6'>7</a>\u001b[0m     data,\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdaps\u001b[39m.\u001b[39mget_indxd_file(index)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000002vscode-remote?line=7'>8</a>\u001b[0m     label,\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdaps\u001b[39m.\u001b[39;49mget_indxd_file(index,\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000002vscode-remote?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (data,label)\n",
      "\u001b[1;32m/home/prasad/AEC/model/CNN_Res.ipynb Cell 2'\u001b[0m in \u001b[0;36mDAPSDatasetHelper.get_indxd_file\u001b[0;34m(self, indx, isLabel)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=23'>24</a>\u001b[0m     \u001b[39m#category=self.keys[np.random.randint(2,len(self.keys))]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=24'>25</a>\u001b[0m     \u001b[39m#consider one category for now\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=25'>26</a>\u001b[0m     category\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys[\u001b[39m4\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=26'>27</a>\u001b[0m data\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39;49mimread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_dict[category][\u001b[39mstr\u001b[39;49m(indx)],\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=27'>28</a>\u001b[0m Id\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict[category][\u001b[39mstr\u001b[39m(indx)]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/AEC/model/CNN_Res.ipynb#ch0000001vscode-remote?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (data,Id)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer,30)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f5c7ac77e593d0994949fe61f06040dedc773eb228097f0a804bc0d2d8d2f83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aec': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
